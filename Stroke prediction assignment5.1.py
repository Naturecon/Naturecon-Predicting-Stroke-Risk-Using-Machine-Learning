{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3a669-3d81-40b6-adbc-6733108b0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stroke_prediction_assignment5_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377138e7-b3d8-4cbf-b3bf-858b3e99860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.1: Predicting Stroke Risk - Experimental Methods\n",
    "\n",
    "# IMPORTING LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# LOADING DATASET:\n",
    "\n",
    "# Using the specific filename provided\n",
    "filename = 'healthcare-dataset-stroke-data[1].csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\" Successfully loaded '{filename}'\")\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: Could not find '{filename}'. Please ensure the file is in the same folder as this notebook.\")\n",
    "\n",
    "# DATA PREPROCESSING AND CLEANING\n",
    "\n",
    "# Drop the 'id' column (not useful for prediction)\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop(columns=['id'])\n",
    "\n",
    "# Handle Missing Values (BMI usually has missing values)\n",
    "# Strategy: Impute with Mean\n",
    "df['bmi'] = df['bmi'].fillna(df['bmi'].mean())\n",
    "\n",
    "# CRITICAL STEP FOR YOUR REPORT: CALCULATE \"X%\" IMBALANCE\n",
    "\n",
    "target_counts = df['stroke'].value_counts(normalize=True)\n",
    "stroke_percentage = target_counts[1] * 100\n",
    "\n",
    "print(\"\\n--- REPORT DATA ---\")\n",
    "print(f\"Class Distribution:\\n{target_counts}\")\n",
    "print(f\" FILL IN YOUR REPORT: Stroke cases represent approximately {stroke_percentage:.2f}% of total samples.\")\n",
    "print(\"-------------------\\n\")\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = df.drop(columns=['stroke'])\n",
    "y = df['stroke']\n",
    "\n",
    "# Identify Categorical and Numerical columns\n",
    "categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "numerical_cols = ['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease']\n",
    "\n",
    "# Create Preprocessing Pipeline\n",
    "# 1. Standardize Numerical Data (Mean=0, Std=1)\n",
    "# 2. One-Hot Encode Categorical Data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply Preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# SPLIT DATA (70% Train, 15% Val, 15% Test)\n",
    "# First split: 70% Train, 30% Temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_processed, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "# Second split: Split Temp into 50% Val, 50% Test (resulting in 15% each total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training Set: {X_train.shape}\")\n",
    "print(f\"Validation Set: {X_val.shape}\")\n",
    "print(f\"Testing Set:  {X_test.shape}\")\n",
    "\n",
    "# MODEL ARCHITECTURE (Based on your Draft)\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer dimensions are automatic based on X_train shape\n",
    "input_dim = X_train.shape[1]\n",
    "print(f\"\\nInput Layer Dimension: {input_dim} neurons\")\n",
    "\n",
    "# Hidden Layer 1: 64 Neurons, ReLU, Dropout 0.3\n",
    "model.add(Dense(64, input_dim=input_dim, activation='relu', name='Hidden_Layer_1'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Hidden Layer 2: 32 Neurons, ReLU, Dropout 0.3\n",
    "model.add(Dense(32, activation='relu', name='Hidden_Layer_2'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Hidden Layer 3: 16 Neurons, ReLU (Bottleneck)\n",
    "model.add(Dense(16, activation='relu', name='Hidden_Layer_3'))\n",
    "\n",
    "# Output Layer: 1 Neuron, Sigmoid (Binary Classification)\n",
    "model.add(Dense(1, activation='sigmoid', name='Output_Layer'))\n",
    "\n",
    "# Compile Model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Attempt to generate Figure 1 image for the report\n",
    "try:\n",
    "    plot_model(model, to_file='Figure1_Model_Architecture.png', \n",
    "               show_shapes=True, show_layer_names=True)\n",
    "    print(\" 'Figure1_Model_Architecture.png' saved successfully. Use this in your report!\")\n",
    "except Exception:\n",
    "    print(\" Could not save model image (Graphviz not installed). You can use the model.summary() output instead.\")\n",
    "\n",
    "# TRAINING PROCEDURE\n",
    "# Early Stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- STARTING TRAINING ---\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# EVALUATION\n",
    "print(\"\\n--- EVALUATING MODEL ---\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generating Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Detailed Metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "roc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"AUC-ROC Score: {roc_score:.4f}\")\n",
    "\n",
    "# VISUALIZATION (Training Evidence)\n",
    "# This plot serves as evidence that training was performed\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save training plots automatically\n",
    "plt.savefig('training_plots.png')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
